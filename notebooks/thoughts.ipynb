{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Progress\n",
    "\n",
    "### EDA\n",
    "The project started with getting the data and the description. From that description I then did some EDA on the data, cleaned and processed the data in the following ways:\n",
    "\n",
    "1. Stop word removal using Spacy\n",
    "2. Lemmatization using Spacy\n",
    "3. Tokenization using Spacy\n",
    "4. Removal of noise tokens such as punctuations and emails\n",
    "\n",
    "Following this, I visualized the data for a little bit and decided to keep the startup descriptions that were between 15-60 tokens in length. This was because I wanted to keep the data that was not too short, and not too long. I experimented with several different lengths until I landed on this range. There are currently 2512 startups in the data that meet this criteria.\n",
    "\n",
    "### Topic Modelling & Clustering\n",
    "\n",
    "Currently I am toying around with several algorithms:\n",
    "1. Vectorization\n",
    "    - TFIDF\n",
    "2. Topic modelling\n",
    "    - LDA\n",
    "    - LSA\n",
    "    - NMF\n",
    "3. Clustering\n",
    "    - DBSCAN\n",
    "    - KM\n",
    "    - HC\n",
    "4. Dimension Reduction\n",
    "    - t-SNE\n",
    "\n",
    "I used a combination of topic modelling and clustering to cluster the startups into industries. I used LDA/LSA/NMF to get the topics from the data, and then used the topics to cluster the data.\n",
    "\n",
    "Through topic modelling, I found that some of the topics were just location-based. For example there were topics that had as top words: india, san francisco, US etc. I decided to remove these words from my dataset by returning to the preprocessing step and removing all entities from the data using Spacy. I then tried topic modelling again, and found that the topics were much better.\n",
    "\n",
    "Following this, I clustered the data using DBSCAN, KM and HC. I tried two approaches:\n",
    "1. Using the top 10 words in each topic as the features\n",
    "2. Using the whole description as the features\n",
    "\n",
    "I then used the best clustering algorithm to cluster the data. The silhouette scores were not so good, so i decided to abort midway and go another route.\n",
    "\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "The next route I tried was using word embeddings. This is the progress on that so far:\n",
    "1. I used BERT to get the embeddings for the data\n",
    "2. The embeddings were pooled (max, avg and concat)\n",
    "\n",
    "I then tried two different things:\n",
    "1. Directly associating the embeddings of the startups with the industries using cosine similarity between the embeddings\n",
    "    - This proved quite poor, and the industries were wrong for many of the startups.\n",
    "    - This could be because the embeddings are not good enough, or because the cosine similarity is not a good metric to use\n",
    "\n",
    "2. Indirectly associating the embeddings of the startups with the industries using cosine similarity between the embeddings obtained from keywords of each industry\n",
    "\n",
    "    - I reasoned that perhaps the industry names were not good enough to be used as a comparison, so I asked gpt4 to generate 10 keywords per industry, and used those to make embeddings for the industries\n",
    "    - The result of this was much better, but still not good enough\n",
    "    - A major problem I saw was that many of the industries are actually very closely related. There is an entry called oceantech and one called marinetech. There is genomics, life sciences, and biotech. I therefore decided maybe it would be best to cluster related industries together, and then use the embeddings to cluster the startups. This step is still not done\n",
    "\n",
    "\n",
    "\n",
    "### Transformers\n",
    "\n",
    "The transformers that i used for the v1 of the project were:\n",
    "1. BERT\n",
    "2. GPT2\n",
    "4. XLNet\n",
    "5. RoBERTa\n",
    "6. DistilBERT\n",
    "7. T5\n",
    "8. ALBERT\n",
    "9. ELECTRA\n",
    "10. BART\n",
    "11. sampathkethineedi/industry-classification\n",
    "\n",
    "The best one was BERT, but still not good enough for using the labels as training data for a NN. I decided there was one of two things that could be done:\n",
    "1. Industries must be defined more concretely for the embeddings to work.\n",
    "This is a problem that I am not sure how to solve without human intervention. This whole project I have tried to keep from scraping new data and just rely on the data that I already have.\n",
    "2. Using sentence embeddings instead of word embeddings.\n",
    "This method can be done using the sentence transformers library. I have used this before but the results were worse than word embeddings. When I used it I had a scientific corpus, so that might be why. I will try it again with the startup data and see if it works better on general english.\n",
    "\n",
    "### Results\n",
    "\n",
    "It worked very well. The sentence embeddings were quite accurate, and I suppose the best pipeline would be one where the sentence embeddings are used to produce the labels for the data, and then the labels are used to train a NN. I will try this out in the next version of the project.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T14:21:07.802429Z",
     "end_time": "2023-04-11T14:21:07.816427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
