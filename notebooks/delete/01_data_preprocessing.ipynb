{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# # # # IMPORTS # # # #\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T23:30:38.057751Z",
     "end_time": "2023-04-11T23:30:44.539212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "with open(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\raw\\startup_dataset.csv', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    raw_startups = pd.read_csv(f)\n",
    "\n",
    "raw_industries = pd.read_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\processed\\industry_dataset_clean.csv', sep='\\t')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T23:30:51.899956Z",
     "end_time": "2023-04-11T23:30:51.943367Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary data exploration\n",
    "\n",
    "In this section, we will explore the data to get a better understanding of the data and the problem we are trying to solve."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate rows:  (0, 3)\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = raw_startups[raw_startups.duplicated()]\n",
    "print(\"number of duplicate rows: \", duplicate_rows.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T22:58:33.192247Z",
     "end_time": "2023-04-11T22:58:33.236250Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "class TextProcessing:\n",
    "    def __init__(self, df: pd.DataFrame = None, industry=False, startup=False):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.startups = startup\n",
    "        self.industries = industry\n",
    "        if startup:\n",
    "            self.startups = df.copy()\n",
    "        elif industry:\n",
    "            self.industries = df.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Please specify if the data is for startups or industries\")\n",
    "\n",
    "    def __iterate_rows(self):\n",
    "        for index, row in tqdm(self.startups.iterrows()):\n",
    "            self.index = index\n",
    "            if self.industries:\n",
    "                self.about_us = row[\"keywords\"]\n",
    "            else:\n",
    "                self.about_us = row[\"cb_description\"]\n",
    "            yield self\n",
    "\n",
    "    def length_range(self, data=None, length_range=(30, 150)):\n",
    "\n",
    "        if data is not None:\n",
    "            if self.industries:\n",
    "                self.industries = data.copy()\n",
    "            else:\n",
    "                self.startups = data.copy()\n",
    "        about_us_lengths = {id: len(about.split()) for id, about in zip(self.startups['id'], self.startups['cb_description']) if length_range[0] < len(about.split()) < length_range[1]}\n",
    "\n",
    "        return about_us_lengths\n",
    "\n",
    "    def remove_non_english_tokens(self, data=None):\n",
    "        if data is not None:\n",
    "            if self.industries:\n",
    "                self.industries = data.copy()\n",
    "            else:\n",
    "                self.startups = data.copy()\n",
    "        english_tokens = []\n",
    "        for description in self.__iterate_rows():\n",
    "            doc = self.nlp(self.about_us)\n",
    "            tokens = [token.text for token in doc if token.lang_ == 'en' and token.is_alpha]\n",
    "            self.about_us = \" \".join(tokens)\n",
    "            english_tokens.append(self.about_us)\n",
    "\n",
    "        if self.startups:\n",
    "            self.startups['cb_description'].replace(to_replace=self.startups['cb_description'].unique(), value=english_tokens, inplace=True)\n",
    "            return self.startups\n",
    "        elif self.industries:\n",
    "            self.industries['keywords'].replace(to_replace=self.industries['keywords'].unique(), value=english_tokens, inplace=True)\n",
    "            return self.industries\n",
    "\n",
    "    def remove_noisy_tokens(self, data=None):\n",
    "        if data is not None:\n",
    "            if self.industries:\n",
    "                self.industries = data.copy()\n",
    "            else:\n",
    "                self.startups = data.copy()\n",
    "        cleaned_about_us = []\n",
    "        for item in self.__iterate_rows():\n",
    "            doc = self.nlp(self.about_us)\n",
    "            tokens = [token.text.lower() for token in doc if\n",
    "                      not token.is_stop\n",
    "                      and not token.is_punct\n",
    "                      and not token.is_space\n",
    "                      and not token.like_num\n",
    "                      and not token.is_digit\n",
    "                      and not token.is_currency\n",
    "                      and not token.is_bracket\n",
    "                      and not token.is_quote\n",
    "                      and not token.is_left_punct\n",
    "                      and not token.is_right_punct\n",
    "                      and not token.like_url\n",
    "                      and not token.like_email]\n",
    "\n",
    "            self.about_us = \" \".join(tokens)\n",
    "            cleaned_about_us.append(self.about_us)\n",
    "        if self.industries:\n",
    "            self.industries['keywords'].replace(to_replace=self.industries['keywords'].unique(), value=cleaned_about_us, inplace=True)\n",
    "            return self.industries\n",
    "        elif self.startups:\n",
    "            self.startups['cb_description'].replace(to_replace=self.startups['cb_description'].unique(), value=cleaned_about_us, inplace=True)\n",
    "            return self.startups\n",
    "\n",
    "    def lemma(self, data=None):\n",
    "        if data is not None:\n",
    "            if self.industries:\n",
    "                self.industries = data.copy()\n",
    "            else:\n",
    "                self.startups = data.copy()\n",
    "        lemmatized_about_us = []\n",
    "        for description in self.__iterate_rows():\n",
    "            doc = self.nlp(self.about_us)\n",
    "            tokens = [token.lemma_ for token in doc]\n",
    "            self.about_us = \" \".join(tokens)\n",
    "            lemmatized_about_us.append(\" \".join(tokens))\n",
    "        if self.industries:\n",
    "            self.industries['keywords'].replace(to_replace=self.industries['keywords'].unique(), value=lemmatized_about_us, inplace=True)\n",
    "            return self.industries\n",
    "        elif self.startups:\n",
    "            self.startups['cb_description'].replace(to_replace=self.startups['cb_description'].unique(), value=lemmatized_about_us, inplace=True)\n",
    "            return self.startups\n",
    "\n",
    "    def delete_ents(self, text):\n",
    "        print(len(text.split()))\n",
    "        doc = self.nlp(text)\n",
    "        ents = [ent.text for ent in doc.ents]\n",
    "        for ent in ents:\n",
    "            text = text.replace(ent, \"\")\n",
    "        print(len(text.split()))\n",
    "        return text\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T23:30:57.041236Z",
     "end_time": "2023-04-11T23:30:57.062102Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m preprocess \u001B[38;5;241m=\u001B[39m TextProcessing(raw_industries, industry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      2\u001B[0m english_tokens \u001B[38;5;241m=\u001B[39m preprocess\u001B[38;5;241m.\u001B[39mremove_non_english_tokens()\n\u001B[0;32m      3\u001B[0m noise_reduce \u001B[38;5;241m=\u001B[39m preprocess\u001B[38;5;241m.\u001B[39mremove_noisy_tokens()\n",
      "Cell \u001B[1;32mIn[9], line 4\u001B[0m, in \u001B[0;36mTextProcessing.__init__\u001B[1;34m(self, df, industry, startup)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: pd\u001B[38;5;241m.\u001B[39mDataFrame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, industry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, startup\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstartups \u001B[38;5;241m=\u001B[39m startup\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindustries \u001B[38;5;241m=\u001B[39m industry\n",
      "\u001B[1;31mNameError\u001B[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess = TextProcessing(raw_industries, industry=True)\n",
    "english_tokens = preprocess.remove_non_english_tokens()\n",
    "noise_reduce = preprocess.remove_noisy_tokens()\n",
    "industries = preprocess.lemma()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T13:37:16.346622Z",
     "end_time": "2023-04-09T13:37:17.796168Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "startups = pd.read_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\processed\\startups_clean_noents.csv')\n",
    "startups.dropna(inplace=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T23:31:04.697795Z",
     "end_time": "2023-04-11T23:31:04.749304Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To inform my assumption that overly small and overly large descriptions are not useful, I will plot the distribution of the lengths of the descriptions. Its important to find a range where the data is not too sparse and not too dense. I will use a range of 30 to 150 words for the descriptions. as a baseline and alter the distributions from there.\n",
    "\n",
    "It follows logically that the cleaned dataset will have a higher bias towards lower description lengths. This is because the cleaning process removes words that are not useful for the model. This is a good thing as it will help the model to focus on the important words in the description.\n",
    "\n",
    "After playing with the range a little I concluded that the range of 15-60 words is the best range for the descriptions. It includes most of the descriptions, in the cleaned dataset (2500), and after looking visually at some entries with 15 words, I concluded that it is enough to understand the type of company (at least for a human). The distribution is also centered around the mean which is a good thing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "datasets = {'raw': raw_startups, 'cleaned': startups}\n",
    "for i, dataset_name in enumerate(datasets.keys()):\n",
    "    dataset = datasets[dataset_name]\n",
    "    about_us_lengths = [len(s.split()) for s in dataset['cb_description'] if 15 < len(s.split()) < 60]\n",
    "    mean, std = norm.fit(about_us_lengths)\n",
    "    ax[i].hist(about_us_lengths, bins=60, density=True, alpha=0.6, color='y')\n",
    "    xmin, xmax = ax[i].get_xlim()\n",
    "    x = np.linspace(xmin, xmax)\n",
    "    p = norm.pdf(x, mean, std)\n",
    "    ax[i].plot(x, p, 'k', linewidth=4)\n",
    "    title = f\"{dataset_name} dataset: mu = {mean.round()},  std = {std.round()} n= {len(about_us_lengths)}\"\n",
    "    ax[i].set_title(title)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "preprocess = TextProcessing(raw_startups, startup=True)\n",
    "about_us_lengths = preprocess.length_range(startups, length_range=(15, 60))\n",
    "\n",
    "ranged_startups = startups[startups['id'].isin(about_us_lengths.keys())]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T23:31:08.695703Z",
     "end_time": "2023-04-11T23:31:10.311525Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I now have a dataframe that contains the cleaned descriptions of 2512 startups that fall within a range of 15-60 words in length per description. Overly long or short ones were excluded, and I imagine for my TFIDF model, I will likely use the first 20-30 words as a proxy for the category. This will limit complexity further as looking at the data, the main concepts seem to be described in the first sentence or two.\n",
    "\n",
    "This is not always the case though but works fine for a v1 model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "ranged_startups.to_csv(path_or_buf=r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\processed\\startup_dataset_clean_1560_range.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T23:31:13.629271Z",
     "end_time": "2023-04-11T23:31:13.663858Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0    id                 name  \\\n0              0  1820                0xKYC   \n2              2  3640         10X-Genomics   \n3              3  9594              111Skin   \n5              5   473              1stdibs   \n6              6  7956                1v1Me   \n...          ...   ...                  ...   \n3992        3992  2649              RoomLab   \n3995        3995  6882               Rosaly   \n3996        3996  4394  Roslin-Technologies   \n3997        3997  1036               Rossum   \n3999        3999  7417            Rotolight   \n\n                                         cb_description  \n0     modular knowledge system identity credential m...  \n2     create revolutionary dna sequence technology h...  \n3     commit positive luxury skincare push boundary ...  \n5     internet company offer marketplace rare desira...  \n6     application allow user play match favorite vid...  \n...                                                 ...  \n3992  believe great interior design accessible avail...  \n3995  give ability manage advance payment request au...  \n3996  mission improve protein production disruptive ...  \n3997  solve key step document base process receive d...  \n3999  rotolight technology company specialize create...  \n\n[2613 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>name</th>\n      <th>cb_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1820</td>\n      <td>0xKYC</td>\n      <td>modular knowledge system identity credential m...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3640</td>\n      <td>10X-Genomics</td>\n      <td>create revolutionary dna sequence technology h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>9594</td>\n      <td>111Skin</td>\n      <td>commit positive luxury skincare push boundary ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>473</td>\n      <td>1stdibs</td>\n      <td>internet company offer marketplace rare desira...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>7956</td>\n      <td>1v1Me</td>\n      <td>application allow user play match favorite vid...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3992</th>\n      <td>3992</td>\n      <td>2649</td>\n      <td>RoomLab</td>\n      <td>believe great interior design accessible avail...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>3995</td>\n      <td>6882</td>\n      <td>Rosaly</td>\n      <td>give ability manage advance payment request au...</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>3996</td>\n      <td>4394</td>\n      <td>Roslin-Technologies</td>\n      <td>mission improve protein production disruptive ...</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>3997</td>\n      <td>1036</td>\n      <td>Rossum</td>\n      <td>solve key step document base process receive d...</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>3999</td>\n      <td>7417</td>\n      <td>Rotolight</td>\n      <td>rotolight technology company specialize create...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2613 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranged_startups"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T23:31:16.025592Z",
     "end_time": "2023-04-11T23:31:16.045209Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "walidvenv",
   "language": "python",
   "display_name": "py3.11 walidvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
