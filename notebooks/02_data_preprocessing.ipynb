{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1 Data Preprocessing\n",
    "\n",
    "Now that we have our data we can preprocess it. We will be using the spacy library to preprocess our data. We will be using the following steps to preprocess our data:\n",
    "\n",
    "1. Remove non-english tokens\n",
    "2. Remove noise from our data which includes things like:\n",
    "    - Stop words\n",
    "    - Punctuation\n",
    "    - Entities like locations and currencies\n",
    "    - Numbers\n",
    "    - URLs\n",
    "    - Emails\n",
    "3. Lemmatize our data\n",
    "4. Remove tokens that are too short or too long. The range i chose was 15-60 tokens per description\n",
    "5. Remove duplicate keywords from our industry dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Cell 1 - Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# # # # IMPORTS # # # #\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T14:37:09.320196Z",
     "end_time": "2023-04-12T14:37:09.329237Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cell 2 - Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "with open(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\raw\\startup_dataset.csv', 'r', encoding='utf-8',\n",
    "          errors='ignore') as f:\n",
    "    raw_startups = pd.read_csv(f)\n",
    "\n",
    "raw_industries = pd.read_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\processed\\industry_dataset_clean.csv',\n",
    "                             sep='\\t')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T13:52:04.981918Z",
     "end_time": "2023-04-12T13:52:05.076656Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cell 3\n",
    "\n",
    "This is the `TextPreprocessing` class that will handle all of our preprocessing. It will take in a dataframe and a boolean value to specify if the data is for startups or industries. It will also have a method to remove tokens that are too short or too long. The range i chose was 15-60 tokens per description. To make this a little cleaner, the dataframe columns `cb_description` and `keywords` on the startups and industry dataframes can be changed to `description` to allow for more uniform handling of the df. I chose not to do this because it would require changing the names of the columns in the other notebooks.\n",
    "\n",
    "This class is stored under `src/data/preprocessing.py`, and can be imported as `from src.data.preprocessing import TextProcessing`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextProcessing:\n",
    "    def __init__(self, df: pd.DataFrame = None, industry=False, startup=False):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.startups = pd.DataFrame([])\n",
    "        self.industries = pd.DataFrame([])\n",
    "        if startup:\n",
    "            self.startups = df.copy()\n",
    "        elif industry:\n",
    "            self.industries = df.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Please specify if the data is for startups or industries\")\n",
    "\n",
    "    def __iterate_rows(self):\n",
    "        df = self.startups if not self.startups.empty else self.industries\n",
    "        for index, row in tqdm(df.iterrows()):\n",
    "            self.index = index\n",
    "            if not self.industries.empty:\n",
    "                self.about_us = row[\"keywords\"]\n",
    "            else:\n",
    "                self.about_us = row[\"cb_description\"]\n",
    "            yield self\n",
    "\n",
    "    def length_range(self, data, length_range=(30, 150)):\n",
    "\n",
    "        self.startups = data.copy()\n",
    "        self.startups.dropna(inplace=True)\n",
    "        for i, row in self.startups.iterrows():\n",
    "            length = len(row['cb_description'].split())\n",
    "            if length < 15 or length > 60:\n",
    "                self.startups.drop(i, inplace=True)\n",
    "\n",
    "        return self.startups\n",
    "\n",
    "    def remove_non_english_tokens(self, data=None):\n",
    "        if data is not None:\n",
    "            if not self.industries.empty:\n",
    "                self.industries = data.copy()\n",
    "            else:\n",
    "                self.startups = data.copy()\n",
    "        english_tokens = []\n",
    "        for description in self.__iterate_rows():\n",
    "            doc = self.nlp(self.about_us)\n",
    "            tokens = [token.text for token in doc if token.lang_ == 'en' and token.is_alpha]\n",
    "            self.about_us = \" \".join(tokens)\n",
    "            english_tokens.append(self.about_us)\n",
    "\n",
    "        if not self.startups.empty:\n",
    "            self.startups['cb_description'].replace(to_replace=self.startups['cb_description'].unique(),\n",
    "                                                    value=english_tokens, inplace=True)\n",
    "            return self.startups\n",
    "\n",
    "        else:\n",
    "            self.industries['keywords'].replace(to_replace=self.industries['keywords'].unique(), value=english_tokens,\n",
    "                                                inplace=True)\n",
    "            return self.industries\n",
    "\n",
    "    def remove_noisy_tokens(self, data=None):\n",
    "        if data is not None:\n",
    "            if not self.industries.empty:\n",
    "                self.industries = data.copy()\n",
    "            else:\n",
    "                self.startups = data.copy()\n",
    "\n",
    "        cleaned_about_us = []\n",
    "        for item in self.__iterate_rows():\n",
    "            doc = self.nlp(self.about_us)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_:\n",
    "                    self.about_us = self.about_us.replace(ent.text, \"\")\n",
    "            cleaned_doc = self.nlp(self.about_us)\n",
    "\n",
    "            tokens = [token.text.lower() for token in cleaned_doc if\n",
    "                      not token.is_stop\n",
    "                      and not token.is_punct\n",
    "                      and not token.is_space\n",
    "                      and not token.like_num\n",
    "                      and not token.is_digit\n",
    "                      and not token.is_currency\n",
    "                      and not token.is_bracket\n",
    "                      and not token.is_quote\n",
    "                      and not token.is_left_punct\n",
    "                      and not token.is_right_punct\n",
    "                      and not token.like_url\n",
    "                      and not token.like_email]\n",
    "\n",
    "            self.about_us = \" \".join(tokens)\n",
    "            cleaned_about_us.append(self.about_us)\n",
    "        if not self.industries.empty:\n",
    "            self.industries['keywords'].replace(to_replace=self.industries['keywords'].unique(), value=cleaned_about_us,\n",
    "                                                inplace=True)\n",
    "            return self.industries\n",
    "        else:\n",
    "            self.startups['cb_description'].replace(to_replace=self.startups['cb_description'].unique(),\n",
    "                                                    value=cleaned_about_us, inplace=True)\n",
    "            return self.startups\n",
    "\n",
    "    def lemma(self, data=None):\n",
    "        if data is not None:\n",
    "            if not self.industries.empty:\n",
    "                self.industries = data.copy()\n",
    "            else:\n",
    "                self.startups = data.copy()\n",
    "        lemmatized_about_us = []\n",
    "        for description in self.__iterate_rows():\n",
    "            doc = self.nlp(self.about_us)\n",
    "            tokens = [token.lemma_ for token in doc]\n",
    "            self.about_us = \" \".join(tokens)\n",
    "            lemmatized_about_us.append(\" \".join(tokens))\n",
    "        if not self.industries.empty:\n",
    "            self.industries['keywords'].replace(to_replace=self.industries['keywords'].unique(),\n",
    "                                                value=lemmatized_about_us, inplace=True)\n",
    "            return self.industries\n",
    "        else:\n",
    "            self.startups['cb_description'].replace(to_replace=self.startups['cb_description'].unique(),\n",
    "                                                    value=lemmatized_about_us, inplace=True)\n",
    "            return self.startups\n",
    "\n",
    "    @staticmethod\n",
    "    def make_keywords_unique(df): # very ugly function but its okay\n",
    "        unique_keywords = set()\n",
    "        for index, row in df.iterrows():\n",
    "            keywords = row['keywords'].split()\n",
    "            unique_keywords.update(set(keywords))\n",
    "        appended_keywords = []\n",
    "        for index, row in df.iterrows():\n",
    "            keywords = [keyword for keyword in row['keywords'].split() if keyword in unique_keywords and keyword not in appended_keywords]\n",
    "            appended_keywords.extend(keywords)\n",
    "            new_keys = ' '.join(keywords)\n",
    "            df.at[index, 'keywords'] = new_keys\n",
    "        return df\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T13:54:11.402316Z",
     "end_time": "2023-04-12T13:54:11.419446Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cell 4\n",
    "\n",
    "This is the start of the preprocessing journey. First you initialize the class with the dataframe you want to preprocess, and explicitly set whether or not it is a startup df or not. This is necessary because of the difference in the column names. Then you can call the methods you want to use. The preprocessed dataframe will be saved as a class attribute, so there is no need to return anything. It is possible to pass a dataframe to the methods, but if you don't, the class attribute will be used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocess = TextProcessing(raw_startups, startup=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cell 5\n",
    "\n",
    "This cell fully preprocesses the data and returns a dataframe with the cleaned text. There will be a progress bar indicating the progress of the preprocessing. This is achieved through the ```tqdm``` library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imran\\AppData\\Local\\Temp\\ipykernel_7304\\1553499897.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, row in tqdm(df.iterrows()):\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d10b7ee0e3f74ef48ed286684d97f65d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66c7f258565045b8891e801c512a1dfd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "422c931f129b4e31b2971843ad46e5f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "        id                 name  \\\n0     1820                0xKYC   \n1     1536                100ms   \n2     3640         10X-Genomics   \n3     9594              111Skin   \n4     4697             1715Labs   \n...    ...                  ...   \n3995  6882               Rosaly   \n3996  4394  Roslin-Technologies   \n3997  1036               Rossum   \n3998  8697            Rotaready   \n3999  7417            Rotolight   \n\n                                         cb_description  \n0     modular knowledge system identity credential m...  \n1     live video infrastructure platform provide sub...  \n2     create revolutionary dna sequence technology h...  \n3     commit positive luxury skincare push boundary ...  \n4            company establish commercialise technology  \n...                                                 ...  \n3995  give ability manage advance payment request au...  \n3996  mission improve protein production disruptive ...  \n3997  solve key step document base process receive d...  \n3998  develop hospitality leisure retail stop shop s...  \n3999  rotolight technology company specialize create...  \n\n[4000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>cb_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1820</td>\n      <td>0xKYC</td>\n      <td>modular knowledge system identity credential m...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1536</td>\n      <td>100ms</td>\n      <td>live video infrastructure platform provide sub...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3640</td>\n      <td>10X-Genomics</td>\n      <td>create revolutionary dna sequence technology h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9594</td>\n      <td>111Skin</td>\n      <td>commit positive luxury skincare push boundary ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4697</td>\n      <td>1715Labs</td>\n      <td>company establish commercialise technology</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>6882</td>\n      <td>Rosaly</td>\n      <td>give ability manage advance payment request au...</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>4394</td>\n      <td>Roslin-Technologies</td>\n      <td>mission improve protein production disruptive ...</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>1036</td>\n      <td>Rossum</td>\n      <td>solve key step document base process receive d...</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>8697</td>\n      <td>Rotaready</td>\n      <td>develop hospitality leisure retail stop shop s...</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>7417</td>\n      <td>Rotolight</td>\n      <td>rotolight technology company specialize create...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess.remove_non_english_tokens()\n",
    "preprocess.remove_noisy_tokens()\n",
    "preprocess.lemma()\n",
    "df = preprocess.length_range(length_range=range(15, 60))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T12:39:14.729047Z",
     "end_time": "2023-04-12T12:45:06.645042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min words per row: 10\n",
      "Max words per row: 17\n",
      "Mean words per row: 12.426229508196721\n",
      "Standard deviation of words per row: 1.3782131969735862\n"
     ]
    }
   ],
   "source": [
    "min_words = df['nr_words'].min()\n",
    "max_words = df['nr_words'].max()\n",
    "mean_words = df['nr_words'].mean()\n",
    "std_dev_words = df['nr_words'].std()\n",
    "\n",
    "print(\"Min words per row:\", min_words)\n",
    "print(\"Max words per row:\", max_words)\n",
    "print(\"Mean words per row:\", mean_words)\n",
    "print(\"Standard deviation of words per row:\", std_dev_words)\n",
    "\n",
    "# or just call df.describe()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T15:12:49.791866Z",
     "end_time": "2023-04-12T15:12:49.796866Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
