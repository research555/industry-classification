{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-tune Sentence Transformer\n",
    "\n",
    "In this notebook, we will go through the process of fine-tuning a sentence transformer. I made this class to make it easier to fine-tune a sentence transformer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cell 2 - FineTuneSentenceTransformer class\n",
    "\n",
    "The class takes in a dataframe of startups and industries and fine-tunes the sentence transformer on the descriptions of the startups and the keywords of the industries. It is a copy of the `FineTuneSentenceTransformer` class in `src/models/training.py`. This code has a bug that i discovered too late. I left the model training, and as an output, you can see the csv file in `models/fine_tuned_sentence_transformer_1/eval/similarity_evaluation_results.csv`. I'm not sure what went wrong, but if need be, I can fix it. Its currently not a priority since this step was more or less a bonus.\n",
    "\n",
    "The basic steps are as follows:\n",
    "\n",
    "- Merge the startups and industries dataframes\n",
    "- Filter out industries that have less than 3 startups\n",
    "- Split the data into train, validation, and test sets, and stratify the split by the keywords\n",
    "- Prepare the examples for the dataloader in the format of sentence1, sentence2\n",
    "- Prepare the dataloader\n",
    "- Prepare the evaluator for the validation set (sentence1, sentence2, score) where score is 1 for all examples\n",
    "- Prepare the loss\n",
    "- Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "class FineTuneSentenceTransfomer:\n",
    "    def __init__(self, startups, industries, label_count_threshold=2, sentence_transformer='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(sentence_transformer)\n",
    "        self.data = self.merge_features(startups, industries, label_count_threshold)\n",
    "        self.loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "\n",
    "    def merge_features(self, startups, industries, label_count_threshold=3):\n",
    "\n",
    "        startups.dropna(inplace=True)\n",
    "        industries.dropna(inplace=True)\n",
    "        merged_df = pd.merge(startups, industries, left_on='industry1', right_on='industry', how='left')\n",
    "        merged_df = merged_df[['cb_description', 'industry1', 'keywords', 'id_y']].dropna()\n",
    "        merged_df = merged_df.groupby('industry1').filter(lambda x : len(x)>label_count_threshold)\n",
    "        merged_df['id_y'] = merged_df['id_y'].astype(int)\n",
    "        self.merged_df = merged_df.rename(columns={'industry1': 'industry', 'cb_description': 'description', 'id_y': 'industry_id'})\n",
    "        return self.merged_df\n",
    "\n",
    "\n",
    "    def split_data(self):\n",
    "\n",
    "        descriptions = self.merged_df['description']\n",
    "        keywords = self.merged_df['keywords']\n",
    "\n",
    "        descriptions_train, self.descriptions_test, keywords_train, self.keywords_test = train_test_split(descriptions, keywords, test_size=0.15, random_state=42, stratify=keywords)\n",
    "        self.descriptions_train, self.descriptions_val, self.keywords_train, self.keywords_val = train_test_split(descriptions_train, keywords_train, test_size=0.1765, random_state=42, stratify=keywords_train)\n",
    "\n",
    "        return self.keywords_train, self.keywords_val, self.keywords_test, self.descriptions_train, self.descriptions_val, self.descriptions_test\n",
    "\n",
    "    def prepare_examples(self, descriptions, keywords):\n",
    "        examples = []\n",
    "        for i in range(len(descriptions)):\n",
    "            examples.append(InputExample(texts=[descriptions[i]], label=keywords[i]))\n",
    "        return examples\n",
    "\n",
    "    def prepare_dataloader(self, examples, batch_size=16):\n",
    "        return DataLoader(examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    def prepare_evaluator(self):\n",
    "\n",
    "        val_sentences1 = [description for description, keyword in zip(self.descriptions_val, self.keywords_val)]\n",
    "        val_sentences2 = [keyword for description, keyword in zip(self.descriptions_val, self.keywords_val)]\n",
    "        val_scores = [1] * len(val_sentences1)\n",
    "        self.evaluator = EmbeddingSimilarityEvaluator(val_sentences1, val_sentences2, val_scores)\n",
    "\n",
    "        return self.evaluator\n",
    "\n",
    "    def prepare_loss(self):\n",
    "        self.loss = losses.CosineSimilarityLoss(self.model)\n",
    "        return self.loss\n",
    "\n",
    "    def train(self,\n",
    "              train_dataloader,\n",
    "              train_loss,\n",
    "              epochs=4,\n",
    "              output_path=r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\models',\n",
    "              warmup_steps=100,\n",
    "              evaluation_steps=100,\n",
    "              weight_decay=0.01,\n",
    "              max_grad_norm=1.0,\n",
    "              save_best_model=True,\n",
    "              checkpoint_save_steps=100,\n",
    "              checkpoint_path=r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\models\\checkpoint'\n",
    "              ):\n",
    "\n",
    "        self.model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                       evaluator=self.evaluator,\n",
    "                       epochs=epochs,\n",
    "                       evaluation_steps=evaluation_steps,\n",
    "                       warmup_steps=warmup_steps,\n",
    "                       output_path=output_path,\n",
    "                       weight_decay=weight_decay,\n",
    "                       checkpoint_path=checkpoint_path,\n",
    "                       checkpoint_save_steps=checkpoint_save_steps,\n",
    "                       max_grad_norm=max_grad_norm,\n",
    "                       save_best_model=save_best_model,\n",
    "                       )\n",
    "        return self.model\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            description  \\\n0     modular knowledge system identity credential m...   \n1     create revolutionary dna sequence technology h...   \n2     commit positive luxury skincare push boundary ...   \n3     internet company offer marketplace rare desira...   \n4     application allow user play match favorite vid...   \n...                                                 ...   \n2706  believe great interior design accessible avail...   \n2707  give ability manage advance payment request au...   \n2708  mission improve protein production disruptive ...   \n2709  solve key step document base process receive d...   \n2710  rotolight technology company specialize create...   \n\n                   industry  \\\n0             cybersecurity   \n1                   biotech   \n2                    beauty   \n3                   fashion   \n4                   esports   \n...                     ...   \n2706        food & beverage   \n2707  professional services   \n2708              longevity   \n2709              logistics   \n2710                 design   \n\n                                               keywords  industry_id  \n0     access malware encryption authentication firew...           13  \n1     vaccine drug cell pharmaceutical genetic engin...           23  \n2     cosmetic makeup fragrance haircare tech skinca...           84  \n3          clothing apparel retail style trend commerce           64  \n4     streaming competition game virtual tournament ...            4  \n...                                                 ...          ...  \n2706              restaurant beverage catering foodtech          120  \n2707  support strategy account human finance consult...            7  \n2708  age lifespan genomic anti healthspan extension...           15  \n2709              ship fulfillment warehousing tracking           45  \n2710  ux product web creative graphic typography bra...           46  \n\n[2291 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>industry</th>\n      <th>keywords</th>\n      <th>industry_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>modular knowledge system identity credential m...</td>\n      <td>cybersecurity</td>\n      <td>access malware encryption authentication firew...</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>create revolutionary dna sequence technology h...</td>\n      <td>biotech</td>\n      <td>vaccine drug cell pharmaceutical genetic engin...</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>commit positive luxury skincare push boundary ...</td>\n      <td>beauty</td>\n      <td>cosmetic makeup fragrance haircare tech skinca...</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>internet company offer marketplace rare desira...</td>\n      <td>fashion</td>\n      <td>clothing apparel retail style trend commerce</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>application allow user play match favorite vid...</td>\n      <td>esports</td>\n      <td>streaming competition game virtual tournament ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2706</th>\n      <td>believe great interior design accessible avail...</td>\n      <td>food &amp; beverage</td>\n      <td>restaurant beverage catering foodtech</td>\n      <td>120</td>\n    </tr>\n    <tr>\n      <th>2707</th>\n      <td>give ability manage advance payment request au...</td>\n      <td>professional services</td>\n      <td>support strategy account human finance consult...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2708</th>\n      <td>mission improve protein production disruptive ...</td>\n      <td>longevity</td>\n      <td>age lifespan genomic anti healthspan extension...</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2709</th>\n      <td>solve key step document base process receive d...</td>\n      <td>logistics</td>\n      <td>ship fulfillment warehousing tracking</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>2710</th>\n      <td>rotolight technology company specialize create...</td>\n      <td>design</td>\n      <td>ux product web creative graphic typography bra...</td>\n      <td>46</td>\n    </tr>\n  </tbody>\n</table>\n<p>2291 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "startups = pd.read_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\tagged\\tagged_with_sentence_transformer.csv')\n",
    "industries = pd.read_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\processed\\industries_clean.csv', sep='\\t')\n",
    "industries.dropna(inplace=True)\n",
    "startups.dropna(inplace=True)\n",
    "merged_df = pd.merge(startups, industries, left_on='industry1', right_on='industry', how='left')\n",
    "merged_df = merged_df.groupby('industry1').filter(lambda x : len(x)>2)\n",
    "merged_df = merged_df[['cb_description', 'industry1', 'keywords', 'id_y']].dropna()\n",
    "merged_df['id_y'] = merged_df['id_y'].astype(int)\n",
    "merged_df = merged_df.rename(columns={'industry1': 'industry', 'cb_description': 'description', 'id_y': 'industry_id'})\n",
    "merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:01:13.402857Z",
     "end_time": "2023-04-12T18:01:13.488455Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split the data into train, validation, and test sets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[101], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m descriptions \u001B[38;5;241m=\u001B[39m merged_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      2\u001B[0m keywords \u001B[38;5;241m=\u001B[39m merged_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeywords\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 4\u001B[0m dataset_train, dataset_test \u001B[38;5;241m=\u001B[39m train_test_split(descriptions, keywords, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.15\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, stratify\u001B[38;5;241m=\u001B[39mkeywords)\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "descriptions = merged_df['description']\n",
    "keywords = merged_df['keywords']\n",
    "\n",
    "descriptions_train, descriptions_test, keywords_train, keywords_test = train_test_split(descriptions, keywords, test_size=0.15, random_state=42, stratify=keywords)\n",
    "descriptions_train, descriptions_val, keywords_train, keywords_val = train_test_split(descriptions_train, keywords_train, test_size=0.1765, random_state=42, stratify=keywords_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:01:21.831475Z",
     "end_time": "2023-04-12T18:01:21.870481Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the data for reproducibility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "descriptions_train.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/descriptions_train.csv', index=False)\n",
    "descriptions_val.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/descriptions_val.csv', index=False)\n",
    "descriptions_test.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/descriptions_test.csv', index=False)\n",
    "\n",
    "keywords_train.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/keywords_train.csv', index=False)\n",
    "keywords_val.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/keywords_val.csv', index=False)\n",
    "keywords_test.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/keywords_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:02:58.554914Z",
     "end_time": "2023-04-12T18:02:58.610154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "(1603, 344, 344)"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make examples\n",
    "\n",
    "train_examples = [InputExample(texts=[desc, kw]) for desc, kw in zip(descriptions_train, keywords_train)]\n",
    "test_examples = [InputExample(texts=[desc, kw]) for desc, kw in zip(descriptions_test, keywords_test)]\n",
    "val_examples = [InputExample(texts=[desc, kw]) for desc, kw in zip(descriptions_val, keywords_val)]\n",
    "\n",
    "len(train_examples), len(test_examples), len(val_examples)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:39:51.824454Z",
     "end_time": "2023-04-12T18:39:51.837462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:39:55.153192Z",
     "end_time": "2023-04-12T18:39:55.168196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "val_sentences1 = [description for description, keyword in zip(descriptions_val, keywords_val)]\n",
    "val_sentences2 = [keyword for description, keyword in zip(descriptions_val, keywords_val)]\n",
    "val_scores = [1] * len(val_sentences1)\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator(val_sentences1, val_sentences2, val_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:39:57.564236Z",
     "end_time": "2023-04-12T18:39:57.572233Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0da1a9f8d46a4465b0a8cb6130483177"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/101 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23494b3c500c46978e1ea5ca0f49bfa4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=4,\n",
    "    evaluation_steps=500,\n",
    "    warmup_steps=2,\n",
    "    output_path=r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\models\\finetuned_sentence_transformer_1'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "python 3.10 sentence transform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
