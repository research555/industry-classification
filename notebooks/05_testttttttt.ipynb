{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-tune Sentence Transformer\n",
    "\n",
    "In this notebook, we will go through the process of fine-tuning a sentence transformer. I made this class to make it easier to fine-tune a sentence transformer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T13:20:27.775962Z",
     "end_time": "2023-04-13T13:20:27.786222Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cell 2 - FineTuneSentenceTransformer class\n",
    "\n",
    "The class takes in a dataframe of startups and industries and fine-tunes the sentence transformer on the descriptions of the startups and the keywords of the industries. It is a copy of the `FineTuneSentenceTransformer` class in `src/models/training.py`. This code has a bug that i discovered too late. I left the model training, and as an output, you can see the csv file in `models/fine_tuned_sentence_transformer_1/eval/similarity_evaluation_results.csv`. I'm not sure what went wrong, but if need be, I can fix it. Its currently not a priority since this step was more or less a bonus.\n",
    "\n",
    "The basic steps are as follows:\n",
    "\n",
    "- Merge the startups and industries dataframes\n",
    "- Filter out industries that have less than 3 startups\n",
    "- Split the data into train, validation, and test sets\n",
    "- Prepare the examples for the dataloader in the format of sentence1, sentence2\n",
    "- Prepare the dataloader\n",
    "- Prepare the evaluator for the validation set (sentence1, sentence2, score) where score is 1 for all examples\n",
    "- Prepare the loss\n",
    "- Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class FineTuneSentenceTransfomer:\n",
    "    def __init__(self, startups, industries, label_count_threshold=2, sentence_transformer='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(sentence_transformer)\n",
    "        self.data = self.merge_features(startups, industries, label_count_threshold)\n",
    "        self.loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "\n",
    "    def merge_features(self, startups, industries, label_count_threshold=3):\n",
    "\n",
    "        merged_df = pd.merge(startups, industries, left_on='industry1', right_on='industry', how='left')\n",
    "        merged_df = merged_df[['cb_description', 'industry1', 'keywords', 'id_y']].dropna()\n",
    "        merged_df = merged_df.groupby('industry1').filter(lambda x : len(x)>label_count_threshold)\n",
    "        merged_df['id_y'] = merged_df['id_y'].astype(int)\n",
    "        self.merged_df = merged_df.rename(columns={'industry1': 'industry', 'cb_description': 'description', 'id_y': 'industry_id'})\n",
    "        return self.merged_df\n",
    "\n",
    "\n",
    "    def split_data(self):\n",
    "\n",
    "        descriptions = self.merged_df['description']\n",
    "        keywords = self.merged_df['keywords']\n",
    "\n",
    "        descriptions_train, self.descriptions_test, keywords_train, self.keywords_test = train_test_split(descriptions, keywords, test_size=0.15, random_state=42, stratify=keywords)\n",
    "        self.descriptions_train, self.descriptions_val, self.keywords_train, self.keywords_val = train_test_split(descriptions_train, keywords_train, test_size=0.1765, random_state=42, stratify=keywords_train)\n",
    "\n",
    "        return self.keywords_train, self.keywords_val, self.keywords_test, self.descriptions_train, self.descriptions_val, self.descriptions_test\n",
    "\n",
    "    def prepare_examples(self, descriptions, keywords):\n",
    "        examples = []\n",
    "        for i in descriptions.index:\n",
    "            examples.append(InputExample(texts=[descriptions[i]], label=keywords[i]))\n",
    "        return examples\n",
    "\n",
    "    def prepare_dataloader(self, examples, batch_size=16):\n",
    "        return DataLoader(examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    def prepare_evaluator(self):\n",
    "\n",
    "        val_sentences1 = [description for description, keyword in zip(self.descriptions_val, self.keywords_val)]\n",
    "        val_sentences2 = [keyword for description, keyword in zip(self.descriptions_val, self.keywords_val)]\n",
    "        val_scores = [1] * len(val_sentences1)\n",
    "        self.evaluator = EmbeddingSimilarityEvaluator(val_sentences1, val_sentences2, val_scores)\n",
    "\n",
    "        return self.evaluator\n",
    "\n",
    "    def prepare_loss(self):\n",
    "        self.loss = losses.CosineSimilarityLoss(self.model)\n",
    "        return self.loss\n",
    "\n",
    "    def train(self,\n",
    "              train_dataloader,\n",
    "              train_loss,\n",
    "              epochs=4,\n",
    "              output_path=r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\models',\n",
    "              warmup_steps=100,\n",
    "              evaluation_steps=100,\n",
    "              weight_decay=0.01,\n",
    "              max_grad_norm=1.0,\n",
    "              save_best_model=True,\n",
    "              checkpoint_save_steps=100,\n",
    "              checkpoint_path=r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\models\\checkpoint'\n",
    "              ):\n",
    "\n",
    "        self.model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                       evaluator=self.evaluator,\n",
    "                       epochs=epochs,\n",
    "                       evaluation_steps=evaluation_steps,\n",
    "                       warmup_steps=warmup_steps,\n",
    "                       output_path=output_path,\n",
    "                       weight_decay=weight_decay,\n",
    "                       checkpoint_path=checkpoint_path,\n",
    "                       checkpoint_save_steps=checkpoint_save_steps,\n",
    "                       max_grad_norm=max_grad_norm,\n",
    "                       save_best_model=save_best_model,\n",
    "                       )\n",
    "        return self.model\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T13:29:19.380552Z",
     "end_time": "2023-04-13T13:29:19.404058Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "startups = pd.read_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\tagged\\tagged_with_sentence_transformer.csv')\n",
    "industries = pd.read_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\processed\\industries_clean.csv', sep='\\t')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T13:29:20.602563Z",
     "end_time": "2023-04-13T13:29:20.633890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Merge the startups and industries dataframes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         description       industry  \\\n0  modular knowledge system identity credential m...  cybersecurity   \n1  create revolutionary dna sequence technology h...        biotech   \n2  commit positive luxury skincare push boundary ...         beauty   \n3  internet company offer marketplace rare desira...        fashion   \n4  application allow user play match favorite vid...        esports   \n\n                                            keywords  industry_id  \n0  access malware encryption authentication firew...           13  \n1  vaccine drug cell pharmaceutical genetic engin...           23  \n2  cosmetic makeup fragrance haircare tech skinca...           84  \n3       clothing apparel retail style trend commerce           64  \n4  streaming competition game virtual tournament ...            4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>industry</th>\n      <th>keywords</th>\n      <th>industry_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>modular knowledge system identity credential m...</td>\n      <td>cybersecurity</td>\n      <td>access malware encryption authentication firew...</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>create revolutionary dna sequence technology h...</td>\n      <td>biotech</td>\n      <td>vaccine drug cell pharmaceutical genetic engin...</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>commit positive luxury skincare push boundary ...</td>\n      <td>beauty</td>\n      <td>cosmetic makeup fragrance haircare tech skinca...</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>internet company offer marketplace rare desira...</td>\n      <td>fashion</td>\n      <td>clothing apparel retail style trend commerce</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>application allow user play match favorite vid...</td>\n      <td>esports</td>\n      <td>streaming competition game virtual tournament ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune = FineTuneSentenceTransfomer(startups, industries)\n",
    "merged_df = fine_tune.merge_features(startups, industries)\n",
    "merged_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T13:29:22.111034Z",
     "end_time": "2023-04-13T13:29:22.480795Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split the data into train, validation, and test sets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1591 341 341\n",
      "1591 341 341\n"
     ]
    }
   ],
   "source": [
    "descriptions = merged_df['description']\n",
    "keywords = merged_df['keywords']\n",
    "\n",
    "keywords_train, keywords_val, keywords_test, descriptions_train, descriptions_val, descriptions_test = fine_tune.split_data()\n",
    "\n",
    "print(len(keywords_train), len(keywords_val), len(keywords_test))\n",
    "print(len(descriptions_train), len(descriptions_val), len(descriptions_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T13:29:24.491282Z",
     "end_time": "2023-04-13T13:29:24.518928Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the data for reproducibility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "descriptions_train.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/descriptions_train.csv', index=False)\n",
    "descriptions_val.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/descriptions_val.csv', index=False)\n",
    "descriptions_test.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/descriptions_test.csv', index=False)\n",
    "\n",
    "keywords_train.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/keywords_train.csv', index=False)\n",
    "keywords_val.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/keywords_val.csv', index=False)\n",
    "keywords_test.to_csv(r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\data\\train_sets/keywords_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:02:58.554914Z",
     "end_time": "2023-04-12T18:02:58.610154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(1591, 341, 341)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make examples\n",
    "train_examples = fine_tune.prepare_examples(descriptions_train, keywords_train)\n",
    "val_examples = fine_tune.prepare_examples(descriptions_val, keywords_val)\n",
    "test_examples = fine_tune.prepare_examples(descriptions_test, keywords_test)\n",
    "\n",
    "\n",
    "len(train_examples), len(val_examples), len(test_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T13:29:38.299055Z",
     "end_time": "2023-04-13T13:29:38.353606Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:39:55.153192Z",
     "end_time": "2023-04-12T18:39:55.168196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "val_sentences1 = [description for description, keyword in zip(descriptions_val, keywords_val)]\n",
    "val_sentences2 = [keyword for description, keyword in zip(descriptions_val, keywords_val)]\n",
    "val_scores = [1] * len(val_sentences1)\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator(val_sentences1, val_sentences2, val_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:39:57.564236Z",
     "end_time": "2023-04-12T18:39:57.572233Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0da1a9f8d46a4465b0a8cb6130483177"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/101 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23494b3c500c46978e1ea5ca0f49bfa4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=4,\n",
    "    evaluation_steps=500,\n",
    "    warmup_steps=2,\n",
    "    output_path=r'C:\\Users\\imran\\DataspellProjects\\WalidCase\\models\\finetuned_sentence_transformer_1'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "python 3.10 sentence transform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
